"""
Module pour effectuer les pr√©dictions
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from utils.preprocessing import create_features, get_feature_columns, load_historical_stats
from utils.model_loader import load_cluster_models


# Charger le mapping store -> cluster (depuis le fichier de clustering)
def load_store_clusters(cluster_features_path='data/cluster_features.pkl'):
    """
    Charge le mapping store -> cluster depuis le fichier de clustering
    
    Returns:
        dict: {store_id: cluster_id}
    """
    try:
        cluster_features = pd.read_pickle(cluster_features_path)
        store_cluster_map = cluster_features.set_index('store')['cluster'].to_dict()
        print(f"‚úì Mapping store->cluster charg√© pour {len(store_cluster_map)} stores")
        return store_cluster_map
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors du chargement du mapping: {e}")
        # Mapping par d√©faut si fichier introuvable
        return {i: i % 4 for i in range(1, 46)}  # 45 stores r√©partis sur 4 clusters


def predict_sales(df_input):
    """
    Effectue les pr√©dictions sur le DataFrame d'entr√©e
    
    Args:
        df_input: DataFrame avec colonnes [store, date, temperature, fuel_Price, 
                  cpi, unemployment, holiday_flag]
    
    Returns:
        DataFrame avec colonnes [store, date, predicted_sales, cluster]
    """
    
    # 1. Charger les statistiques historiques pour imputation
    historical_stats = load_historical_stats()
    
    # 2. Supprimer weekly_sales si elle existe (forcer l'imputation pour pr√©diction)
    df_for_prediction = df_input.copy()
    if 'weekly_sales' in df_for_prediction.columns:
        print("‚ÑπÔ∏è  Colonne 'weekly_sales' d√©tect√©e - Elle sera ignor√©e pour la pr√©diction")
        print("    Les lags seront imput√©s avec les statistiques historiques")
        df_for_prediction = df_for_prediction.drop(columns=['weekly_sales'])
    
    # 3. Appliquer le feature engineering
    print("\nüìä Feature engineering...")
    df_features = create_features(df_for_prediction, historical_stats)
    
    if df_features.empty:
        raise ValueError("Aucune donn√©e disponible apr√®s le feature engineering")
    
    # 3. Charger le mapping store -> cluster
    store_cluster_map = load_store_clusters()
    df_features['cluster'] = df_features['store'].map(store_cluster_map)
    
    # V√©rifier que tous les stores ont un cluster
    missing_clusters = df_features['cluster'].isna().sum()
    if missing_clusters > 0:
        print(f"‚ö†Ô∏è  {missing_clusters} stores sans cluster assign√© - utilisation du cluster 0")
        df_features['cluster'] = df_features['cluster'].fillna(0).astype(int)
    
    # 4. Essayer de charger les mod√®les par cluster, sinon utiliser un mod√®le global
    print("\nü§ñ Chargement du mod√®le...")
    feature_cols = get_feature_columns()
    
    try:
        # Tenter de charger les mod√®les de cluster individuels
        cluster_models = load_cluster_models()
        use_cluster_models = True
        print(f"‚úì {len(cluster_models)} mod√®les de cluster charg√©s")
    except FileNotFoundError:
        # Utiliser un mod√®le global unique
        print("‚ö†Ô∏è  Mod√®les de cluster non trouv√©s, entra√Ænement d'un mod√®le global...")
        use_cluster_models = False
        
        # Charger les donn√©es d'entra√Ænement
        try:
            train_data = pd.read_pickle('data/train.pkl')
            print(f"‚úì Donn√©es d'entra√Ænement charg√©es: {len(train_data)} lignes")
            
            # Entra√Æner un mod√®le LightGBM global
            model = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
            model.fit(train_data[feature_cols], train_data['weekly_sales'])
            print("‚úì Mod√®le global entra√Æn√©")
        except Exception as e:
            raise ValueError(f"Impossible de charger les donn√©es d'entra√Ænement: {e}")
    
    # 5. Pr√©dire
    print("\nüéØ Pr√©diction...")
    
    if use_cluster_models:
        # Pr√©dire avec les mod√®les par cluster
        predictions = []
        
        for cluster_id in sorted(df_features['cluster'].unique()):
            df_cluster = df_features[df_features['cluster'] == cluster_id].copy()
            
            if cluster_id not in cluster_models:
                print(f"‚ö†Ô∏è  Mod√®le pour cluster {cluster_id} introuvable - skip")
                continue
            
            cluster_model = cluster_models[cluster_id]
            y_pred = cluster_model.predict(df_cluster[feature_cols])
            
            df_cluster['predicted_sales'] = y_pred
            predictions.append(df_cluster[['store', 'date', 'predicted_sales', 'cluster']])
            print(f"  Cluster {cluster_id}: {len(df_cluster)} pr√©dictions")
        
        df_predictions = pd.concat(predictions).reset_index(drop=True)
    else:
        # Pr√©dire avec le mod√®le global
        y_pred = model.predict(df_features[feature_cols])
        df_features['predicted_sales'] = y_pred
        df_predictions = df_features[['store', 'date', 'predicted_sales', 'cluster']].copy()
        print(f"  {len(df_predictions)} pr√©dictions avec mod√®le global")
    
    print(f"\n‚úì {len(df_predictions)} pr√©dictions g√©n√©r√©es avec succ√®s")
    
    return df_predictions


def get_summary_stats(df_predictions):
    """
    Calcule des statistiques descriptives sur les pr√©dictions
    
    Returns:
        dict: Statistiques agr√©g√©es
    """
    return {
        'total_predictions': len(df_predictions),
        'mean_sales': df_predictions['predicted_sales'].mean(),
        'median_sales': df_predictions['predicted_sales'].median(),
        'min_sales': df_predictions['predicted_sales'].min(),
        'max_sales': df_predictions['predicted_sales'].max(),
        'total_sales': df_predictions['predicted_sales'].sum(),
        'unique_stores': df_predictions['store'].nunique(),
        'unique_dates': df_predictions['date'].nunique(),
    }
